---
phase: 04-health-alerting
plan: 03
type: execute
wave: 2
depends_on: ["04-01", "04-02"]
files_modified:
  - deploy/compose/grafana/provisioning/alerting/alert-rules.yaml
  - deploy/compose/grafana/provisioning/alerting/contact-points.yaml
  - deploy/compose/grafana/provisioning/alerting/notification-policies.yaml
  - deploy/compose/grafana/provisioning/datasources/datasources.yml
  - deploy/compose/docker-compose.dev.yml
  - tests/Dhadgar.Notifications.Tests/Alerting/AlertThrottlerTests.cs
  - tests/Dhadgar.Notifications.Tests/Alerting/AlertDispatcherTests.cs
autonomous: true

must_haves:
  truths:
    - "Grafana starts with provisioned alert rules for error rate spikes"
    - "Grafana has contact points configured for Discord webhook"
    - "Error rate above threshold triggers alert"
    - "Alert throttler prevents duplicate alerts within window"
  artifacts:
    - path: "deploy/compose/grafana/provisioning/alerting/alert-rules.yaml"
      provides: "Provisioned alert rules for error rate monitoring"
      contains: "apiVersion: 1"
    - path: "deploy/compose/grafana/provisioning/alerting/contact-points.yaml"
      provides: "Discord and email contact points"
      contains: "contactPoints"
    - path: "deploy/compose/grafana/provisioning/alerting/notification-policies.yaml"
      provides: "Alert routing policies"
      contains: "policies"
    - path: "tests/Dhadgar.Notifications.Tests/Alerting/AlertThrottlerTests.cs"
      provides: "Unit tests for throttling logic"
      min_lines: 40
  key_links:
    - from: "alert-rules.yaml"
      to: "Loki datasource"
      via: "datasourceUid reference"
      pattern: "datasourceUid.*loki"
    - from: "notification-policies.yaml"
      to: "contact-points.yaml"
      via: "receiver name reference"
      pattern: "receiver.*dhadgar-alerts"
---

<objective>
Configure Grafana alerting with provisioned rules for error rate monitoring and add tests for alerting infrastructure.

Purpose: Complete the alerting story by enabling Grafana to detect error rate spikes across all services and route alerts to Discord. Also validate the application-level alerting infrastructure with unit tests.

Output: Grafana provisioned with alert rules that trigger when error rate exceeds threshold, plus comprehensive tests for alert throttling.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-health-alerting/04-RESEARCH.md

# Prior plan summaries for alerting infrastructure
@.planning/phases/04-health-alerting/04-01-SUMMARY.md
@.planning/phases/04-health-alerting/04-02-SUMMARY.md

# Existing Grafana provisioning
@deploy/compose/grafana/provisioning/datasources/datasources.yml
@deploy/compose/docker-compose.dev.yml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Grafana alert provisioning YAML files</name>
  <files>deploy/compose/grafana/provisioning/alerting/alert-rules.yaml, deploy/compose/grafana/provisioning/alerting/contact-points.yaml, deploy/compose/grafana/provisioning/alerting/notification-policies.yaml</files>
  <action>
Create the alerting directory and three YAML files for Grafana provisioning.

Create `deploy/compose/grafana/provisioning/alerting/alert-rules.yaml`:

```yaml
# Provisioned alert rules for Dhadgar services
# These rules query Loki for error patterns and trigger alerts
apiVersion: 1

groups:
  - orgId: 1
    name: dhadgar-error-alerts
    folder: Dhadgar Alerts
    interval: 1m
    rules:
      # Alert when error rate exceeds threshold across any service
      - uid: provisioned-error-rate-spike
        title: High Error Rate Detected
        condition: C
        data:
          # Query Loki for error count in last 5 minutes
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: loki
            model:
              expr: 'sum(count_over_time({service_name=~"Dhadgar.*"} |~ "(?i)level.*error|level.*critical" [5m]))'
              queryType: range
          # Reduce to single value
          - refId: B
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              expression: A
              type: reduce
              reducer: last
          # Threshold check: alert if > 10 errors in 5 minutes
          - refId: C
            datasourceUid: __expr__
            model:
              expression: B
              type: threshold
              conditions:
                - evaluator:
                    params:
                      - 10
                    type: gt
        noDataState: OK
        execErrState: Alerting
        for: 2m
        annotations:
          summary: 'High error rate detected across Dhadgar services'
          description: 'More than 10 errors in the last 5 minutes. Check Loki logs for details.'
        labels:
          severity: warning
          team: platform

      # Alert on critical errors (immediate, no wait)
      - uid: provisioned-critical-error
        title: Critical Error Detected
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 60
              to: 0
            datasourceUid: loki
            model:
              expr: 'sum(count_over_time({service_name=~"Dhadgar.*"} |~ "(?i)level.*critical" [1m]))'
              queryType: range
          - refId: B
            relativeTimeRange:
              from: 60
              to: 0
            datasourceUid: __expr__
            model:
              expression: A
              type: reduce
              reducer: last
          - refId: C
            datasourceUid: __expr__
            model:
              expression: B
              type: threshold
              conditions:
                - evaluator:
                    params:
                      - 0
                    type: gt
        noDataState: OK
        execErrState: Alerting
        for: 0s
        annotations:
          summary: 'Critical error in Dhadgar service'
          description: 'A critical-level error was logged. Immediate attention required.'
        labels:
          severity: critical
          team: platform

      # Alert on service health check failures
      - uid: provisioned-health-check-failure
        title: Service Health Check Failed
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: loki
            model:
              expr: 'sum(count_over_time({service_name=~"Dhadgar.*"} |~ "health.*check.*fail|readiness.*unhealthy" [5m]))'
              queryType: range
          - refId: B
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              expression: A
              type: reduce
              reducer: last
          - refId: C
            datasourceUid: __expr__
            model:
              expression: B
              type: threshold
              conditions:
                - evaluator:
                    params:
                      - 3
                    type: gt
        noDataState: OK
        execErrState: Alerting
        for: 1m
        annotations:
          summary: 'Service health check failures detected'
          description: 'Multiple health check failures in the last 5 minutes. Service may be degraded.'
        labels:
          severity: warning
          team: platform
```

Create `deploy/compose/grafana/provisioning/alerting/contact-points.yaml`:

```yaml
# Provisioned contact points for alert notifications
# Configure DISCORD_WEBHOOK_URL and ALERT_EMAIL environment variables
apiVersion: 1

contactPoints:
  - orgId: 1
    name: dhadgar-alerts
    receivers:
      # Discord webhook for real-time alerts
      - uid: provisioned-discord-alerts
        type: discord
        settings:
          url: ${DISCORD_WEBHOOK_URL:-}
          use_discord_username: false
          avatar_url: ""
          message: |
            **{{ .Status | toUpper }}** - {{ .CommonLabels.alertname }}

            {{ range .Alerts }}
            **Summary:** {{ .Annotations.summary }}
            **Description:** {{ .Annotations.description }}
            **Severity:** {{ .Labels.severity }}
            **Started:** {{ .StartsAt.Format "2006-01-02 15:04:05 UTC" }}
            {{ end }}
        disableResolveMessage: false

      # Email for audit trail (optional, requires SMTP config in Grafana)
      - uid: provisioned-email-alerts
        type: email
        settings:
          addresses: ${ALERT_EMAIL_ADDRESSES:-}
          singleEmail: true
        disableResolveMessage: false
```

Create `deploy/compose/grafana/provisioning/alerting/notification-policies.yaml`:

```yaml
# Provisioned notification policies for alert routing
apiVersion: 1

policies:
  - orgId: 1
    receiver: dhadgar-alerts
    group_by:
      - alertname
      - severity
    group_wait: 30s
    group_interval: 5m
    repeat_interval: 4h
    routes:
      # Critical alerts: shorter repeat interval
      - receiver: dhadgar-alerts
        matchers:
          - severity = critical
        group_wait: 10s
        group_interval: 1m
        repeat_interval: 1h
        continue: false

      # Warning alerts: standard intervals
      - receiver: dhadgar-alerts
        matchers:
          - severity = warning
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 4h
        continue: false
```
  </action>
  <verify>
1. Files exist at the correct paths
2. YAML syntax is valid: `python3 -c "import yaml; yaml.safe_load(open('deploy/compose/grafana/provisioning/alerting/alert-rules.yaml'))"`
3. Repeat for contact-points.yaml and notification-policies.yaml
  </verify>
  <done>Grafana alerting configuration files created with error rate rules, Discord contact point, and notification policies.</done>
</task>

<task type="auto">
  <name>Task 2: Update datasources and docker-compose for alerting</name>
  <files>deploy/compose/grafana/provisioning/datasources/datasources.yml, deploy/compose/docker-compose.dev.yml</files>
  <action>
Update `deploy/compose/grafana/provisioning/datasources/datasources.yml` to add explicit UIDs that alert rules reference:

```yaml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    uid: prometheus
    isDefault: true
  - name: Loki
    type: loki
    access: proxy
    url: http://loki:3100
    uid: loki
```

Update `deploy/compose/docker-compose.dev.yml` to add environment variables for Grafana alerting. Find the grafana service and add environment variables:

```yaml
  grafana:
    image: grafana/grafana:11.4.0
    ports:
      - "3000:3000"
    environment:
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_DISABLE_LOGIN_FORM=true
      # Alert webhook configuration (set actual URL in .env or override)
      - DISCORD_WEBHOOK_URL=${DISCORD_WEBHOOK_URL:-}
      - ALERT_EMAIL_ADDRESSES=${ALERT_EMAIL_ADDRESSES:-}
      # Enable unified alerting
      - GF_UNIFIED_ALERTING_ENABLED=true
      - GF_ALERTING_ENABLED=false
    volumes:
      - ./grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
      - ./grafana/provisioning/alerting:/etc/grafana/provisioning/alerting
      - grafana-data:/var/lib/grafana
    depends_on:
      - prometheus
      - loki
```

Make sure the alerting directory is mounted. If there's already a grafana service definition, add/update only the needed parts (environment variables for alerting and the alerting volume mount).

Note: The docker-compose file may have different structure - adapt as needed while preserving existing configuration.
  </action>
  <verify>
1. `docker compose -f deploy/compose/docker-compose.dev.yml config` validates without errors
2. Grafana service has the alerting volume mounted
3. Datasource UIDs match what alert-rules.yaml references
  </verify>
  <done>Docker compose updated to provision Grafana alerting configuration and pass webhook environment variables.</done>
</task>

<task type="auto">
  <name>Task 3: Add unit tests for alert throttling and dispatcher</name>
  <files>tests/Dhadgar.Notifications.Tests/Alerting/AlertThrottlerTests.cs, tests/Dhadgar.Notifications.Tests/Alerting/AlertDispatcherTests.cs</files>
  <action>
Create `tests/Dhadgar.Notifications.Tests/Alerting/AlertThrottlerTests.cs`:

```csharp
using Dhadgar.Notifications.Alerting;
using FluentAssertions;

namespace Dhadgar.Notifications.Tests.Alerting;

public sealed class AlertThrottlerTests
{
    [Fact]
    public void ShouldSend_FirstAlert_ReturnsTrue()
    {
        // Arrange
        var throttler = new AlertThrottler(TimeSpan.FromMinutes(5));
        var alert = CreateAlert("TestService", "Test Alert");

        // Act
        var result = throttler.ShouldSend(alert);

        // Assert
        result.Should().BeTrue();
    }

    [Fact]
    public void ShouldSend_DuplicateWithinWindow_ReturnsFalse()
    {
        // Arrange
        var throttler = new AlertThrottler(TimeSpan.FromMinutes(5));
        var alert = CreateAlert("TestService", "Test Alert");

        // Act
        throttler.ShouldSend(alert); // First call
        var result = throttler.ShouldSend(alert); // Duplicate within window

        // Assert
        result.Should().BeFalse();
    }

    [Fact]
    public void ShouldSend_DifferentAlerts_BothReturnTrue()
    {
        // Arrange
        var throttler = new AlertThrottler(TimeSpan.FromMinutes(5));
        var alert1 = CreateAlert("Service1", "Alert 1");
        var alert2 = CreateAlert("Service2", "Alert 2");

        // Act
        var result1 = throttler.ShouldSend(alert1);
        var result2 = throttler.ShouldSend(alert2);

        // Assert
        result1.Should().BeTrue();
        result2.Should().BeTrue();
    }

    [Fact]
    public void ShouldSend_SameServiceDifferentTitle_BothReturnTrue()
    {
        // Arrange
        var throttler = new AlertThrottler(TimeSpan.FromMinutes(5));
        var alert1 = CreateAlert("TestService", "Alert Type A");
        var alert2 = CreateAlert("TestService", "Alert Type B");

        // Act
        var result1 = throttler.ShouldSend(alert1);
        var result2 = throttler.ShouldSend(alert2);

        // Assert
        result1.Should().BeTrue();
        result2.Should().BeTrue();
    }

    [Fact]
    public void ShouldSend_SameAlertDifferentException_BothReturnTrue()
    {
        // Arrange
        var throttler = new AlertThrottler(TimeSpan.FromMinutes(5));
        var alert1 = CreateAlert("TestService", "Error", "NullReferenceException");
        var alert2 = CreateAlert("TestService", "Error", "ArgumentException");

        // Act
        var result1 = throttler.ShouldSend(alert1);
        var result2 = throttler.ShouldSend(alert2);

        // Assert
        result1.Should().BeTrue();
        result2.Should().BeTrue();
    }

    [Fact]
    public void ShouldSend_AfterWindowExpires_ReturnsTrue()
    {
        // Arrange - Use very short window for testing
        var throttler = new AlertThrottler(TimeSpan.FromMilliseconds(50));
        var alert = CreateAlert("TestService", "Test Alert");

        // Act
        throttler.ShouldSend(alert); // First call
        Thread.Sleep(100); // Wait for window to expire
        var result = throttler.ShouldSend(alert); // After window

        // Assert
        result.Should().BeTrue();
    }

    [Theory]
    [InlineData(AlertSeverity.Warning)]
    [InlineData(AlertSeverity.Error)]
    [InlineData(AlertSeverity.Critical)]
    public void ShouldSend_AllSeverities_FirstAlertReturnsTrue(AlertSeverity severity)
    {
        // Arrange
        var throttler = new AlertThrottler(TimeSpan.FromMinutes(5));
        var alert = new AlertMessage
        {
            Title = "Test Alert",
            Message = "Test message",
            Severity = severity,
            ServiceName = "TestService"
        };

        // Act
        var result = throttler.ShouldSend(alert);

        // Assert
        result.Should().BeTrue();
    }

    private static AlertMessage CreateAlert(
        string serviceName,
        string title,
        string? exceptionType = null)
    {
        return new AlertMessage
        {
            Title = title,
            Message = $"Test message for {title}",
            Severity = AlertSeverity.Error,
            ServiceName = serviceName,
            ExceptionType = exceptionType
        };
    }
}
```

Create `tests/Dhadgar.Notifications.Tests/Alerting/AlertDispatcherTests.cs`:

```csharp
using Dhadgar.Notifications.Alerting;
using Dhadgar.Notifications.Discord;
using Dhadgar.Notifications.Email;
using Microsoft.Extensions.Logging.Abstractions;
using NSubstitute;
using FluentAssertions;

namespace Dhadgar.Notifications.Tests.Alerting;

public sealed class AlertDispatcherTests
{
    private readonly IDiscordWebhook _mockDiscord;
    private readonly IEmailSender _mockEmail;
    private readonly AlertThrottler _throttler;
    private readonly AlertDispatcher _dispatcher;

    public AlertDispatcherTests()
    {
        _mockDiscord = Substitute.For<IDiscordWebhook>();
        _mockEmail = Substitute.For<IEmailSender>();
        _throttler = new AlertThrottler(TimeSpan.FromMinutes(5));
        _dispatcher = new AlertDispatcher(
            _mockDiscord,
            _mockEmail,
            _throttler,
            NullLogger<AlertDispatcher>.Instance);
    }

    [Fact]
    public async Task DispatchAsync_SendsToDiscordAndEmail()
    {
        // Arrange
        var alert = CreateAlert();

        // Act
        await _dispatcher.DispatchAsync(alert);

        // Assert
        await _mockDiscord.Received(1).SendAlertAsync(alert, Arg.Any<CancellationToken>());
        await _mockEmail.Received(1).SendAlertEmailAsync(alert, Arg.Any<CancellationToken>());
    }

    [Fact]
    public async Task DispatchAsync_ThrottledAlert_DoesNotSend()
    {
        // Arrange
        var alert = CreateAlert();

        // Act
        await _dispatcher.DispatchAsync(alert); // First call
        await _dispatcher.DispatchAsync(alert); // Throttled

        // Assert
        await _mockDiscord.Received(1).SendAlertAsync(alert, Arg.Any<CancellationToken>());
        await _mockEmail.Received(1).SendAlertEmailAsync(alert, Arg.Any<CancellationToken>());
    }

    [Fact]
    public async Task DispatchAsync_DifferentAlerts_SendsBoth()
    {
        // Arrange
        var alert1 = CreateAlert("Service1");
        var alert2 = CreateAlert("Service2");

        // Act
        await _dispatcher.DispatchAsync(alert1);
        await _dispatcher.DispatchAsync(alert2);

        // Assert
        await _mockDiscord.Received(2).SendAlertAsync(Arg.Any<AlertMessage>(), Arg.Any<CancellationToken>());
        await _mockEmail.Received(2).SendAlertEmailAsync(Arg.Any<AlertMessage>(), Arg.Any<CancellationToken>());
    }

    [Fact]
    public async Task DispatchAsync_DiscordFails_StillSendsEmail()
    {
        // Arrange
        var alert = CreateAlert();
        _mockDiscord
            .SendAlertAsync(Arg.Any<AlertMessage>(), Arg.Any<CancellationToken>())
            .Returns(Task.FromException(new HttpRequestException("Discord unreachable")));

        // Act
        Func<Task> act = () => _dispatcher.DispatchAsync(alert);

        // Assert - Should not throw (parallel execution with Task.WhenAll catches exceptions)
        // Note: Actual implementation may need adjustment based on how errors are handled
        await act.Should().NotThrowAsync();
        await _mockEmail.Received(1).SendAlertEmailAsync(alert, Arg.Any<CancellationToken>());
    }

    [Fact]
    public async Task DispatchAsync_AlertWithTraceContext_PassesToChannels()
    {
        // Arrange
        var alert = new AlertMessage
        {
            Title = "Test Alert",
            Message = "Test message",
            Severity = AlertSeverity.Error,
            ServiceName = "TestService",
            TraceId = "abc123",
            CorrelationId = "corr456"
        };

        // Act
        await _dispatcher.DispatchAsync(alert);

        // Assert
        await _mockDiscord.Received(1).SendAlertAsync(
            Arg.Is<AlertMessage>(a => a.TraceId == "abc123" && a.CorrelationId == "corr456"),
            Arg.Any<CancellationToken>());
    }

    [Fact]
    public async Task DispatchAsync_CancellationRequested_PassesCancellationToken()
    {
        // Arrange
        var alert = CreateAlert();
        using var cts = new CancellationTokenSource();
        cts.Cancel();

        // Act & Assert
        await _mockDiscord.Received(0).SendAlertAsync(Arg.Any<AlertMessage>(), Arg.Any<CancellationToken>());
    }

    private static AlertMessage CreateAlert(string serviceName = "TestService")
    {
        return new AlertMessage
        {
            Title = "Test Alert",
            Message = "This is a test alert message",
            Severity = AlertSeverity.Error,
            ServiceName = serviceName
        };
    }
}
```

Make sure the test project has necessary package references. Check if NSubstitute and FluentAssertions are already referenced; if not, they should be since they're in Directory.Packages.props and used in other test projects.
  </action>
  <verify>
1. `dotnet build tests/Dhadgar.Notifications.Tests`
2. `dotnet test tests/Dhadgar.Notifications.Tests --filter "FullyQualifiedName~AlertThrottlerTests"` - all tests pass
3. `dotnet test tests/Dhadgar.Notifications.Tests --filter "FullyQualifiedName~AlertDispatcherTests"` - all tests pass
  </verify>
  <done>Unit tests validate throttling behavior and dispatcher wiring.</done>
</task>

</tasks>

<verification>
1. Grafana starts successfully with alert provisioning: `docker compose -f deploy/compose/docker-compose.dev.yml up grafana -d`
2. Grafana UI shows "Dhadgar Alerts" folder with 3 alert rules
3. Contact point "dhadgar-alerts" is visible in Grafana alerting configuration
4. All unit tests pass: `dotnet test tests/Dhadgar.Notifications.Tests`
5. Docker compose validates without errors
</verification>

<success_criteria>
- Grafana provisioning files exist in deploy/compose/grafana/provisioning/alerting/
- alert-rules.yaml defines error-rate-spike, critical-error, and health-check-failure rules
- contact-points.yaml configures Discord webhook contact point
- notification-policies.yaml routes alerts by severity
- Docker compose mounts alerting provisioning directory
- Datasources have explicit UIDs matching alert rule references
- AlertThrottlerTests verify throttling window behavior
- AlertDispatcherTests verify multi-channel dispatch
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/04-health-alerting/04-03-SUMMARY.md`
</output>
